{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# COMP3411/9418 21T0 Assignment 2\n",
    "\n",
    "- Lecturer: Anna Trofimova\n",
    "- School of Computer Science and Engineering, UNSW Sydney\n",
    "- Last Update 26th January at 07:00am, 2021\n",
    "$$\n",
    "% macros\n",
    "\\newcommand{\\indep}{\\perp \\!\\!\\!\\perp}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student:\n",
    "Hoya Lee, z5226463"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission\n",
    "This interactive notebook contains the instructions to complete assignment 2; You should submit this notebook with the code and answers in one single file in .ipybn format with the name assignment2.ipybn. **Write your name and zID in the cell above** (to edit the markdown text double-click the cell).\n",
    "\n",
    "There is a maximum file size cap of 5MB, so make sure your submission does not exceed this size. The submitted notebook should contain all your source code and answers. You can add new cells and use markdown text to organise and explain your implementation/answer.\n",
    "\n",
    "Submit your files using give. On a CSE Linux machine, type the following on the command-line:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ give cs3411 ass2 assignment2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submission deadline is **3rd February at 11.59pm, 2021.** This is a hard deadline, no extentions will be granted (it is not our decision, it is the reality of the summer term courses)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Late Submission Policy \n",
    "The penalty is set at 20% per late day. This is a ceiling penalty, so if your submission is marked 12/20 and it was submitted two days late, you still get 12/20. If you submit 5 days later, then the penalty is 100% and your mark will be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plagiarism\n",
    "This is an individual assignment. Remember that **all** work submitted for this assignment must be your own **individual** work and no code\n",
    "sharing or copying is allowed. You may use code from the Internet only with suitable attribution\n",
    "of the source in your program. **Do not use public code repositories as your code might be copied.** Keep in mind that sharing parts of assignment solutions is a form of plagiarism. All submitted assignments will be run through plagiarism detection software to detect similarities to other submissions. You should carefully read the UNSW policy on academic integrity and plagiarism. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "\n",
    "Imagine that you have been hired as a Data Scientist by Amazon, and your first task is to evaluate customer sentiment towards their products. Many online stores selling Amazon products provide their customers with an option to leave a review, but they might not have a rating system, or the customers might choose to leave a review without rating. However, you still want to use these reviews in your report. Thus, you need to develop a reliable model that can automatically assign setiment given a review.\n",
    "\n",
    "To develop your model, you have been given a collection of customer reviews on products like Alexa Echo, Echo dots, Alexa Firesticks etc. with their corresponding ratings. The ratings vary from 1 to 5, but to simplify the problem, you will consider reviews with ratings 1 & 2 to have negative sentiment, with 3 having neutral sentiment,  and 4 & 5 having positive sentiment.\n",
    "* Negative: 1 & 2\n",
    "* Neutral: 3\n",
    "* Positive: 4 & 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "When working with a Jupyter notebook, you can edit the \\*.py files either in the Jupyter interface (in your browser) or with your favorite editor (e.g., PyCharm). Whenever you save a \\*.py file, the notebook will reload their content directly.\n",
    "\n",
    "**Do not create new markdown cells, if you want to comment on something then use Raw NBConvert cells.**\n",
    "\n",
    "Below are the libraries that you can use (and need) in this assignment. If you want to use a library that is not in the list then send us an email to confirm (use course email). \n",
    "\n",
    "Run the code below to import the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport nltk\\nimport ssl\\n\\ntry:\\n    _create_unverified_https_context = ssl._create_unverified_context\\nexcept AttributeError:\\n    pass\\nelse:\\n    ssl._create_default_https_context = _create_unverified_https_context\\n\\nnltk.download()\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import graphviz\n",
    "from graphviz import Source\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import re\n",
    "import copy\n",
    "\n",
    "\n",
    "#  If you experience problems with downloading stopwords, uncomment and run the code below.\n",
    "#  It will launch NLTK Downloader application so you can download stopwords corpora manually.\n",
    "'''\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Data [2 marks]\n",
    "\n",
    "In this part of the assignment you need to import the Amazon reviews dataset and analyse its main properties.\n",
    "\n",
    "#### Task 1.1\n",
    "Import the dataset from *amazon_alexa.tsv* file, save it into the variable *data* and change the rating labels as follow: \n",
    " {1: 'negative', 2: 'negative', 3: 'neutral', 4: 'positive', 5: 'positive'}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "data = pd.read_csv('amazon_alexa.tsv', sep='\\t')\n",
    "\n",
    "data.rating.replace([1,2,3,4,5], ['negative', 'negative', 'neutral', 'positive', 'positive'], inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to plot the data distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['rating'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give a simple description of the class frequency distribution across the full dataset. What do you notice about the distribution? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "From the graph plotted graph above, it can be deemed it is skewed heavily towards \"positive\". This means that the data has a significant amount of positive reviews compared to negative and neutral, and hence the dataset is biased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.2\n",
    "\n",
    "To reduce computation time in this task, your will use only the 1000 most frequent tokens from the vocabulary as attributes. Run the code below to convert the input text samples into a document-term matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>!!</th>\n",
       "      <th>&amp;</th>\n",
       "      <th>&amp;#34;Alexa,</th>\n",
       "      <th>&amp;#34;Things</th>\n",
       "      <th>(which</th>\n",
       "      <th>,</th>\n",
       "      <th>-</th>\n",
       "      <th>--</th>\n",
       "      <th>.</th>\n",
       "      <th>...</th>\n",
       "      <th>worth</th>\n",
       "      <th>would</th>\n",
       "      <th>wouldn't</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yet</th>\n",
       "      <th>yet.</th>\n",
       "      <th>you</th>\n",
       "      <th>you're</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   !  !!  &  &#34;Alexa,  &#34;Things  (which  ,  -  --  .  ...  worth  would  \\\n",
       "0  0   0  0            0            0       0  0  0   0  0  ...      0      0   \n",
       "1  0   0  0            0            0       0  0  0   0  0  ...      0      0   \n",
       "2  0   0  0            0            0       0  0  0   0  0  ...      0      0   \n",
       "3  0   0  0            0            0       0  0  0   0  0  ...      0      0   \n",
       "4  0   0  0            0            0       0  0  0   0  0  ...      0      0   \n",
       "\n",
       "   wouldn't  year  years  yet  yet.  you  you're  your  \n",
       "0         0     0      0    0     0    0       0     0  \n",
       "1         0     0      0    0     0    0       0     0  \n",
       "2         0     0      0    0     0    2       0     0  \n",
       "3         0     0      0    0     0    0       0     0  \n",
       "4         0     0      0    0     0    0       0     0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer=lambda x: x.split(), max_features=1000)\n",
    "x_dense = vectorizer.fit_transform(data['verified_reviews'])\n",
    "x_sparse = pd.DataFrame.sparse.from_spmatrix(x_dense, columns=vectorizer.get_feature_names())\n",
    "x_sparse.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was written in the helper notebook that the original input representation is bad. Explain why a document-term matrix is a more suitable representation for machine learning tasks with text data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Document-term matrix describes the frequency of the terms that occur in a collection of documents. The matrix basically shows which documents contain which terms and how many times they appear. Moreoever, as document-term matrix shows the frequency of each words, it can filter out some word-like sequence of characters such as emojis and URLs. It can sort the sequence according to the frequency as well. Furthermore, machine learning tasks usually take in numerical values as input data and hence document-term matrix would be more appropriate in order to aid classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two variables *x_dense* and *x_sparce* both represent input data in a document-term form. Explain which one is preferable and why."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your answer\n",
    "Between the two variables x_dense and x_sparse, s_sparse is more preferable. This is because the data collected for the document-term matrix is sparse, or contains a lot of 0 elements. In this case, a traditional matrix would be unsuitable as the number of rows and columns that would store the value 0 would make it very inefficient. A sparse matrix only stores non-zero entries of the document-term matrix and hence is significantly more efficient in terms of space and time complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Decision Tree [2 marks]\n",
    "\n",
    "In this part you need to evaluate how classifier hyperparameters affect their performance. Consider a Decision Tree classifier that splits attributes based on the lowest entropy. For performance evaluation, assume that only the top 1000 most frequent tokens are used as attributes. The first 60% of samples should constitute the traing set while the remaining 40% of samples should constitute the test set.\n",
    "\n",
    "#### Task 2.1\n",
    "In the cell below place your code that computes performance measures for decision trees with different depth limits. <br> \n",
    "Consider the following cases: max_depth = 5, 10, 100, 200, None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree with depth 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.45      0.05      0.08       108\n",
      "     neutral       0.00      0.00      0.00        67\n",
      "    positive       0.86      0.99      0.92      1085\n",
      "\n",
      "    accuracy                           0.85      1260\n",
      "   macro avg       0.44      0.34      0.34      1260\n",
      "weighted avg       0.78      0.85      0.80      1260\n",
      "\n",
      "Decision tree with depth 10\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.29      0.10      0.15       108\n",
      "     neutral       0.10      0.03      0.05        67\n",
      "    positive       0.87      0.96      0.91      1085\n",
      "\n",
      "    accuracy                           0.84      1260\n",
      "   macro avg       0.42      0.36      0.37      1260\n",
      "weighted avg       0.78      0.84      0.80      1260\n",
      "\n",
      "Decision tree with depth 100\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.31      0.26      0.28       108\n",
      "     neutral       0.10      0.06      0.07        67\n",
      "    positive       0.89      0.92      0.90      1085\n",
      "\n",
      "    accuracy                           0.82      1260\n",
      "   macro avg       0.43      0.41      0.42      1260\n",
      "weighted avg       0.79      0.82      0.81      1260\n",
      "\n",
      "Decision tree with depth 200\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.31      0.31      0.31       108\n",
      "     neutral       0.06      0.03      0.04        67\n",
      "    positive       0.89      0.91      0.90      1085\n",
      "\n",
      "    accuracy                           0.81      1260\n",
      "   macro avg       0.42      0.42      0.42      1260\n",
      "weighted avg       0.79      0.81      0.80      1260\n",
      "\n",
      "Decision tree with depth None\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.29      0.26      0.27       108\n",
      "     neutral       0.10      0.06      0.07        67\n",
      "    positive       0.89      0.92      0.90      1085\n",
      "\n",
      "    accuracy                           0.82      1260\n",
      "   macro avg       0.42      0.41      0.42      1260\n",
      "weighted avg       0.79      0.82      0.80      1260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "\n",
    "y = data['rating']\n",
    "maxdepth = [5,10,100,200, None]\n",
    "keys = [i for i in x_sparse]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_sparse, y, train_size=0.6, test_size=0.4, random_state=1, shuffle=False) # 70% training and 30% test\n",
    "for i in maxdepth:\n",
    "    \n",
    "    clf = tree.DecisionTreeClassifier(criterion='entropy',max_depth = i)\n",
    "\n",
    "    model = clf.fit(X_train, y_train)\n",
    "    \n",
    "    ytestpredicted = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    print(f\"Decision tree with depth {i}\")\n",
    "    print(classification_report(y_test, ytestpredicted))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2.2\n",
    "In the cell below explain any difference in performance, and comment on metrics in relation to the depth limit."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your answer\n",
    "From the metrics above, decision trees with shallower (lower) depths had a slightly better accuracy score compared to the deeper trees (higher depth). However, as seen from the table above, there is no significant different between the metrics of the decision trees of different depths other than accuracy due to the biased nature of the dataset. Because most samples have a \"positive\" review, the data is heavily skewed and that explains the highh value for precision, recall, and f1-score for the \"positive\" class. As a consequence, the \"neutral\" and \"negative\" classes have a lower value compared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Pre-processing [3 marks]\n",
    "\n",
    "In this part, you need to evaluate the effect that the pre-processing of input features has on the performance of three classifiers - Decision Tree (DT), Multinomial Naive Bayes (MNB), and  Artificial Neural Networks (ANN). For a Decision Tree classifier, use the parameters from Part2 but do not limit the depth. For a Artificial Neural Network classifier, set the number of hidden neurons to 200.\n",
    "\n",
    "For the performance evaluation, assume that the first 60% of samples constitutes the traing set while the remaining 40% of samples constitutes the test set. All tokens in the vocabulary should be used as attributes.\n",
    "Consider the following scenarios:\n",
    "1. No data pre-processing\n",
    "2. Removal of URLs and tokens that are not made of strings of letters, numbers, or symbols {’, #, @, $} delimited by spaces.\n",
    "3. Apply (2) and make all tokens lowercase.\n",
    "4. Apply (2), (3) and remove all stop words.\n",
    "\n",
    "#### Task 3.1\n",
    "In the cell below place your code that reflects the four scenarios of feature pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# case 1\n",
    "def preprocessing_1(x):\n",
    "    return x\n",
    "\n",
    "# case 2\n",
    "def preprocessing_2(x):\n",
    "    symbols = [\"'\", \"#\", \"@\", \"$\"]\n",
    "    series = []\n",
    "    for i in x:\n",
    "        wordsArr = i.split()\n",
    "        tmpSentence = []\n",
    "        for j in range(len(wordsArr)):\n",
    "            # Remove URLs\n",
    "            if(wordsArr[j].endswith('.com') or 'www.' in wordsArr[j] or 'http' in wordsArr[j]):\n",
    "                continue\n",
    "            # Remove tokens that are not letters, numbers or valid symbols\n",
    "            tmpStr = ''\n",
    "            for k in range(len(wordsArr[j])):\n",
    "                if wordsArr[j][k].isalnum() or wordsArr[j][k] in symbols :\n",
    "                    tmpStr = tmpStr +  wordsArr[j][k]\n",
    "                else:\n",
    "                    continue\n",
    "            tmpSentence.append(tmpStr)\n",
    "        series.append(' '.join(tmpSentence))\n",
    "    series = pd.Series(series)\n",
    "    return series\n",
    "\n",
    "# case 3\n",
    "def preprocessing_3(x):\n",
    "    newSeries = preprocessing_2(x)\n",
    "    series = []\n",
    "    for i in newSeries:\n",
    "        lower = i.lower()\n",
    "        series.append(lower)\n",
    "    newSeries = pd.Series(series)\n",
    "    return newSeries\n",
    "\n",
    "# case 4\n",
    "def preprocessing_4(x):\n",
    "    newSeries = preprocessing_3(x)\n",
    "    \n",
    "    cachedStopWords = stopwords.words('english')\n",
    "    series = []\n",
    "    for i in newSeries:\n",
    "        series.append(' '.join([word for word in i.split() if word not in cachedStopWords]))\n",
    "    series = pd.Series(series)\n",
    "    return series\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3.2 \n",
    "\n",
    "In the cell below, place your code that trains and tests all three classifiers. Pre-process your data according on scenario #4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.34      0.31      0.32       108\n",
      "     neutral       0.05      0.03      0.04        67\n",
      "    positive       0.88      0.92      0.90      1085\n",
      "\n",
      "    accuracy                           0.82      1260\n",
      "   macro avg       0.43      0.42      0.42      1260\n",
      "weighted avg       0.79      0.82      0.81      1260\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.45      0.09      0.15       108\n",
      "     neutral       0.14      0.03      0.05        67\n",
      "    positive       0.87      0.98      0.92      1085\n",
      "\n",
      "    accuracy                           0.85      1260\n",
      "   macro avg       0.49      0.37      0.37      1260\n",
      "weighted avg       0.79      0.85      0.81      1260\n",
      "\n",
      "Artificial Neural Network\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.23      0.32       108\n",
      "     neutral       0.29      0.03      0.05        67\n",
      "    positive       0.88      0.98      0.93      1085\n",
      "\n",
      "    accuracy                           0.87      1260\n",
      "   macro avg       0.56      0.41      0.43      1260\n",
      "weighted avg       0.82      0.87      0.83      1260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code   \n",
    "x = preprocessing_4(data['verified_reviews'])\n",
    "\n",
    "countvec =  CountVectorizer(analyzer=lambda x: x.split())\n",
    "\n",
    "dense = countvec.fit_transform(data['verified_reviews'])\n",
    "sparse = pd.DataFrame.sparse.from_spmatrix(dense, columns=countvec.get_feature_names())\n",
    "\n",
    "y = data['rating']\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(sparse, y, train_size=0.6, test_size=0.4, random_state=1, shuffle=False) # 60% training and 40% test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Decision Tree\")\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy',max_depth = None)\n",
    "model = clf.fit(xtrain, ytrain)\n",
    "ytestpredicted = model.predict(xtest)\n",
    "print(classification_report(ytest, ytestpredicted))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Multinomial Naive Bayes\")\n",
    "clf = MultinomialNB()\n",
    "model = clf.fit(xtrain, ytrain)    \n",
    "ytestpredicted = model.predict(xtest)\n",
    "print(classification_report(ytest, ytestpredicted))\n",
    "\n",
    "\n",
    "print(\"Artificial Neural Network\")\n",
    "clf = MLPClassifier(random_state=1, hidden_layer_sizes=200)\n",
    "model = clf.fit(xtrain, ytrain)\n",
    "ytestpredicted = model.predict(xtest)\n",
    "print(classification_report(ytest, ytestpredicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3.3\n",
    "\n",
    "Evaluate the effect each pre-processing scenario has on the performance for the three models. In each case. compute the performance metrics. Write your answer in the cell below. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 [5 marks]\n",
    "\n",
    "In this part, you need to evaluate how the number of features and classes affects the performance of the three classifiers - Decision Tree (DT), Multinomial Naive Bayes (MNB), and Artificial Neural Networks (ANN). For a Decision Tree classifier, use the parameters from Part 2 but do not limit the depth. For a Artificial Neural Network classifier set the number of hidden neurons to 200.\n",
    "\n",
    "For the performance evaluation, assume that the first 60% of samples constitute the traing set while the remaining 40% of samples constitute the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.1\n",
    "\n",
    "Consider the following scenarios for attributes:\n",
    "\n",
    "1. Attributes are all tokens in the vocabulary\n",
    "2. Attributes are the top 1000 most frequent tokens in the vocabulary\n",
    "3. Attributes are the top 100 most frequent tokens in the vocabulary\n",
    "4. Attributes are the top 10 most frequent tokens in the vocabulary\n",
    "\n",
    "In the cell below, place your code that returns a document-term representation given the number of attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "\n",
    "def limit_attributes(x, n_attributes):\n",
    "    vectorizer = CountVectorizer(analyzer=lambda x: x.split(), max_features=n_attributes)\n",
    "\n",
    "    x_dense = vectorizer.fit_transform(x)\n",
    "    x_sparse = pd.DataFrame.sparse.from_spmatrix(x_dense, columns=vectorizer.get_feature_names())\n",
    "\n",
    "    return x_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.2 \n",
    "\n",
    "In the cell below, place your code that trains and tests all three classifiers. Limit the number of attributes according on scenario #4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.22      0.15      0.18       108\n",
      "     neutral       0.04      0.03      0.03        67\n",
      "    positive       0.87      0.91      0.89      1085\n",
      "\n",
      "    accuracy                           0.80      1260\n",
      "   macro avg       0.38      0.36      0.37      1260\n",
      "weighted avg       0.77      0.80      0.79      1260\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.02      0.04       108\n",
      "     neutral       0.00      0.00      0.00        67\n",
      "    positive       0.86      1.00      0.93      1085\n",
      "\n",
      "    accuracy                           0.86      1260\n",
      "   macro avg       0.62      0.34      0.32      1260\n",
      "weighted avg       0.83      0.86      0.80      1260\n",
      "\n",
      "Artificial Neural Network\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.27      0.04      0.07       108\n",
      "     neutral       0.10      0.01      0.03        67\n",
      "    positive       0.87      0.99      0.92      1085\n",
      "\n",
      "    accuracy                           0.85      1260\n",
      "   macro avg       0.41      0.35      0.34      1260\n",
      "weighted avg       0.78      0.85      0.80      1260\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoya/ass/.env/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sparse = limit_attributes(data['verified_reviews'], 10)\n",
    "y = data['rating']\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(sparse, y, train_size=0.6, test_size=0.4, random_state=1, shuffle=False) # 60% training and 40% test\n",
    "print(\"Decision Tree\")\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy',max_depth = None)\n",
    "model = clf.fit(xtrain, ytrain)\n",
    "ytestpredicted = model.predict(xtest)\n",
    "print(classification_report(ytest, ytestpredicted))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Multinomial Naive Bayes\")\n",
    "clf = MultinomialNB()\n",
    "model = clf.fit(xtrain, ytrain)    \n",
    "ytestpredicted = model.predict(xtest)\n",
    "print(classification_report(ytest, ytestpredicted))\n",
    "\n",
    "\n",
    "print(\"Artificial Neural Network\")\n",
    "clf = MLPClassifier(random_state=1, hidden_layer_sizes=200)\n",
    "model = clf.fit(xtrain, ytrain)\n",
    "ytestpredicted = model.predict(xtest)\n",
    "print(classification_report(ytest, ytestpredicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.3\n",
    "\n",
    "Evaluate the effect each scenario has on the performance for the three models based on the performance metrics. Explain the differences in the performance. Write your answer in the cell below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.4\n",
    "\n",
    "Remove all samples with neutral sentiment from the dataset. Train and test all three models on this new dataset, computing the preformance measures as well. Assume that the first 60% of samples constitute the traing set while the remaining 40% of samples constitute the test set. All tokens in the vocabulary should be used as attributes.\n",
    "\n",
    "Place your code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.37      0.26      0.30       109\n",
      "    positive       0.93      0.96      0.94      1091\n",
      "\n",
      "    accuracy                           0.89      1200\n",
      "   macro avg       0.65      0.61      0.62      1200\n",
      "weighted avg       0.88      0.89      0.88      1200\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.34      0.12      0.18       109\n",
      "    positive       0.92      0.98      0.95      1091\n",
      "\n",
      "    accuracy                           0.90      1200\n",
      "   macro avg       0.63      0.55      0.56      1200\n",
      "weighted avg       0.87      0.90      0.88      1200\n",
      "\n",
      "Artificial Neural Network\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.25      0.37       109\n",
      "    positive       0.93      0.99      0.96      1091\n",
      "\n",
      "    accuracy                           0.92      1200\n",
      "   macro avg       0.84      0.62      0.67      1200\n",
      "weighted avg       0.91      0.92      0.91      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "noNeutral = data[data.rating != 'neutral']\n",
    "y = noNeutral['rating']\n",
    "sparse = limit_attributes(noNeutral['verified_reviews'], None)\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(sparse, y, train_size=0.6, test_size=0.4, random_state=1, shuffle=False) # 60% training and 40% test\n",
    "\n",
    "\n",
    "print(\"Decision Tree\")\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy',max_depth = None)\n",
    "model = clf.fit(xtrain, ytrain)\n",
    "ytestpredicted = model.predict(xtest)\n",
    "print(classification_report(ytest, ytestpredicted))\n",
    "\n",
    "\n",
    "print(\"Multinomial Naive Bayes\")\n",
    "clf = MultinomialNB()\n",
    "model = clf.fit(xtrain, ytrain)    \n",
    "ytestpredicted = model.predict(xtest)\n",
    "print(classification_report(ytest, ytestpredicted))\n",
    "\n",
    "\n",
    "print(\"Artificial Neural Network\")\n",
    "clf = MLPClassifier(random_state=1, hidden_layer_sizes=200)\n",
    "model = clf.fit(xtrain, ytrain)\n",
    "ytestpredicted = model.predict(xtest)\n",
    "print(classification_report(ytest, ytestpredicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.5\n",
    "Compare these results to the results obtained in scenario #1 (part 4). Is there any difference in the metrics for either of the classes (i.e. consider positive and negative classes individually)?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Comparing the results from scenario 1 and the dataset with out the neutral sentiment, there was a significant difference between the computed metrics. First of all, accuracy was significantly improved when neutral was removed. The said datasaid had at least 0.9 accuracy for all classifiers. On the other hand, dataset from scenario 1 had a slightly less accuracy rate. Moreover, the dataset without neutral sentiment had a higher macro and weighted average overall like the accuracy rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 [8 marks]\n",
    "\n",
    "In this part you need to develop your own method (pipeline) for sentiment analysis. You can create new code cells (to place your code) and Raw NBConvert cells (to descibe the steps you are taking to develop your model, or make a comment).\n",
    "\n",
    "You can use either Decision Tree, Multinomial Naive Bayes (MNB), or Artificial Neural Network (ANN) classifiers. If you use parameters for your classifier, the you need to justify the values that you chose for them in writing or in coding & writing (if it is based on evaluation resuts) - similar to Part 2.\n",
    "\n",
    "Since this part of the assignment carries the highest mark, the explanation of your method and its evaluation should be descibed in detail.\n",
    "\n",
    "Optionally: you can also make plots to visualize your findings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
